{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fall 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract alpha for ridge\n",
    "alpha[(np.mean(model['RegLR'].cv_values_, axis=0).argmin())]\n",
    "\n",
    "## silhoutte score is not always knife plot, those silhoutte plots/diagrams\n",
    "silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k]\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(c, silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "### Here we use kmeans for clustering but do pca ONLY for visualiztion\n",
    "X = df2\n",
    "y = df2['data_type']\n",
    "le = LabelEncoder()\n",
    "\n",
    "X['data_type'] = le.fit_transform(X['data_type'])\n",
    "y = le.transform(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Final choice of number of clusters\n",
    "n_clusters = 4\n",
    "clusterer = KMeans(n_clusters=n_clusters, random_state=seed)\n",
    "cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "# Calculate PCA\n",
    "PCA_transformer = PCA(3)\n",
    "PCA_data = PCA_transformer.fit_transform(X)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "fig.add_axes(ax)\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    ax.scatter(PCA_data[cluster_labels==i, 0], PCA_data[cluster_labels==i, 1], PCA_data[cluster_labels==i, 2], alpha=0.8, label='Cluster %i' % (i+1))\n",
    "\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "plt.title('PCA-transformed plot for %i clusters' % n_clusters)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winter 2022\n",
    "## Task 1\n",
    "- plot kde and histogram\n",
    "- MinMaxScaler() fit_transform to trainX but only transform to test\n",
    "- Does linearRegression and prints coeffs matched to columns\n",
    "- Does RidgeCV and prints best alpha using the .alpha_ attribute\n",
    "- it is asked to reprot squared res and CI with CLT, they do that by calculating MSE with built-in fcn over test set, this is our value. For the distribution they use manual coding of squared residuals over all test set and go from there. \n",
    "- they choose ridgeCV over linearRegression bc it is more parsimonious \n",
    "\n",
    "## Task 2\n",
    "- They create interaction terms, they do so by instantiating PolyNomialFeatures with given params, then set X = poly.fit_transform(X)\n",
    "- instantiates GridSearchCV for XGBoost, note the important params are the estimator, param_dict, cv, scoring.\n",
    "- studies this with PolyFeature, and uses the names attribute to separate the linear from interaction terms (i.e. a susbset of X is given)\n",
    "\n",
    "## Task 3\n",
    "- In clustering, dummy label is not separated out, they are all normalized with MinMaxScaler() as question asked\n",
    "- KElbowVisualizer for finding elbow\n",
    "- loop over the k clusters plus using SilhouetteVisualizer for knife plots\n",
    "- analysis of knife plot takes the vertiacl red line (mean) and the overlap of individual knives into account\n",
    "- KMeans() is run to get predictions (cluster labels), and then UMAP used for visualization\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winter 2021\n",
    "## Task 1\n",
    "- exclude outliers by conditional indexing using $\\le$ or $\\ge$.\n",
    "- does a linear regression of degree 2 where feature extraction and fitting to model is done via pipeline\n",
    "- same thing is done with a higher order polynomial and asked whch model should we choose\n",
    "- uses ColumnTransformer to apply different poly deg to different columns of X and runs the model on transformed X\n",
    "## Task 2\n",
    "- Another model with GridSearchCV for XGBoosting and ROC curve as evaluation\n",
    "- Asks some questions about model fairness\n",
    "\n",
    "## Task 3\n",
    "- Normalize X and hand over to Kmeans\n",
    "- Do a loop over clusters and do knife plots and store silhoutte scores\n",
    "- Decide on k clusters, and do PCA for visualization purposes, color with respect to predicted labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall 2020\n",
    "## Task 1\n",
    "- drop all NaN rows \n",
    "- builds a LogisticRegression Model and performs cross_val_score on them\n",
    "- Drop one variable in X at a time, and perform cross val with neg log\n",
    "\n",
    "## Task 2\n",
    "- Asks for L1 regularization and plotting of Lasso Path\n",
    "- Asks to do a plot with Lasso path again but this time with mean CV scores with neg likelihood criteria\n",
    "- Fit using best lambda found and do a AUC plot\n",
    "- Analysis of the plot: given a certain sensitivity what is the best achievable specificity?\n",
    "- Do a CI for specificity \n",
    "\n",
    "## Task 4:\n",
    "- BaggingClassifiers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3018ec948c2f097dac52d5fcdb1a30ae92fa49919476c7ddb718ffa59f845b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
