{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week 1\n",
    "import scipy.stats as ss \n",
    "import scipy.optimize as so\n",
    "\n",
    "\n",
    "# week 2\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# week 3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# week 4\n",
    "from scipy.stats import t\n",
    "import scipy.stats as ss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# week 5\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, ShuffleSplit, LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # need these when use Classes to create custom transformers\n",
    "from plotnine import *\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rename\n",
    "my_data = my_data.rename(columns = {'x1':'pred_1', 'x2':'pred_2'})\n",
    "\n",
    "### to see if data is balanced\n",
    "rice.CLASS.value_counts()\n",
    "\n",
    "### converting string to integer\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "### data types\n",
    "df.dtypes\n",
    "\n",
    "### null/NaN values\n",
    "df.isnull().any()\n",
    "\n",
    "### for both null and data types\n",
    "df.info()\n",
    "\n",
    "### fill null values\n",
    "df= df.fillna('M')\n",
    "\n",
    "### stats summary\n",
    "df.describe()\n",
    "\n",
    "### one hot encoding with pd.get_dummmies\n",
    "df = pd.get_dummies(df,columns=[\"Sex\", \"BP\",\"Cholesterol\"])\n",
    "model_data = pd.get_dummies(model_data, drop_first=True) \n",
    "\n",
    "### Setting binary values\n",
    "df.loc[-df[\"Drug\"].isin(['DrugY']),\"Drug\"]=\"0\"\n",
    "df.loc[df[\"Drug\"].isin(['DrugY']),\"Drug\"]=\"1\"\n",
    "\n",
    "### splitting for test and train\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=0.25,random_state=11)\n",
    "\n",
    "### correlation - note that c is of type pandas dataframe\n",
    "c = df3.corr()\n",
    "\n",
    "### for sorting\n",
    "df.sort_values(ascending=True)\n",
    "\n",
    "### example of indexing with multiple conditions\n",
    "dataLE.loc[(dataLE['work_rate_att'] == 0) | (dataLE['work_rate_att'] == 1)]\n",
    "\n",
    "### example usage LabelEncoder()\n",
    "dataLE = data.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "dataLE['work_rate_att']= label_encoder.fit_transform(dataLE['work_rate_att'])\n",
    "print(dataLE.loc[dataLE['work_rate_att'] == 0].work_rate_att.count())\n",
    "print(dataLE.loc[dataLE['work_rate_att'] == 1].work_rate_att.count())\n",
    "print(dataLE.loc[dataLE['work_rate_att'] == 2].work_rate_att.count())\n",
    "print(label_encoder.classes_)\n",
    "dataLE.head()\n",
    "\n",
    "### example usage OneHotEncoder()\n",
    "# Turn category into numeric variables\n",
    "# One-Hot Encoding with sklearn OneHotEncoder()\n",
    "dataOHE = data.copy()\n",
    "ohe = OneHotEncoder()\n",
    "#reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object \n",
    "X = ohe.fit_transform(dataOHE.work_rate_att.values.reshape(-1,1)).toarray()\n",
    "#To add this back into the original dataframe \n",
    "dfOneHot = pd.DataFrame(X, columns = [\"work_rate_att_\"+str(int(i)) for i in range(3)]) \n",
    "dataOHE = pd.concat([dataOHE, dfOneHot], axis=1)\n",
    "#droping the cwork_rate_att column \n",
    "dataOHE = dataOHE.drop(['work_rate_att'], axis=1)\n",
    "print(dataOHE[(dataOHE['work_rate_att_0'] == 1.0)].work_rate_att_0.count())\n",
    "print(dataOHE[(dataOHE['work_rate_att_1'] == 1.0)].work_rate_att_1.count())\n",
    "print(dataOHE[(dataOHE['work_rate_att_2'] == 1.0)].work_rate_att_2.count())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### baseline accuracy\n",
    "A  = rice.CLASS[rice['CLASS']=='Osmancik'].count() # majority class\n",
    "B = rice.CLASS[rice['CLASS']=='Cammeo'].count()\n",
    "\n",
    "baselineacc = A/(A+B) # the classifier you train must beat this number else no point in training one\n",
    "\n",
    "print('Baseline Accuracy: '+str((baselineacc*100).round(1))+'%')\n",
    "\n",
    "### label-base accuracy, precision, specificty and recall\n",
    "def compute_performance(yhat, y, classes):\n",
    "    # First, get tp, tn, fp, fn\n",
    "    tp = sum(np.logical_and(yhat == classes[1], y == classes[1]))\n",
    "    tn = sum(np.logical_and(yhat == classes[0], y == classes[0]))\n",
    "    fp = sum(np.logical_and(yhat == classes[1], y == classes[0]))\n",
    "    fn = sum(np.logical_and(yhat == classes[0], y == classes[1]))\n",
    "\n",
    "    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Precision\n",
    "    # \"Of the ones I labeled +, how many are actually +?\"\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    # Recall\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    recall = tp / (tp + fn)    \n",
    "    \n",
    "    # Sensitivity\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    sensitivity = recall\n",
    "    \n",
    "    # Specificity\n",
    "    # \"Of all the - in the data, how many do I correctly label?\"\n",
    "    specificity = tn / (fp + tn)\n",
    "    \n",
    "    # Print results\n",
    "    \n",
    "    print(\"Accuracy:\",round(acc,3),\"Recall:\",round(recall,3),\"Precision:\",round(precision,3),\n",
    "          \"Sensitivity:\",round(sensitivity,3),\"Specificity:\",round(specificity,3))\n",
    "\n",
    "compute_performance(ytest_hat, ytest, ricelr.classes_)\n",
    "\n",
    "### ROC ranking-based metric for classification\n",
    "fpr, tpr, _ = roc_curve(ytest, ytest_prob[:,1], pos_label=ricelr.classes_[1]) # 2nd arg: ranking score, 3rd arg: \"Osmancik\"\n",
    "ax =sns.lineplot(x=fpr,y=tpr)\n",
    "plt.plot([0,1], [0,1], \"r--\")\n",
    "ax.set_xlabel(\"FP Rate\")\n",
    "ax.set_ylabel(\"TP Rate\")\n",
    "\n",
    "### auc\n",
    "auc(fpr,tpr)\n",
    "\n",
    "### auc score from ytrue and ypred\n",
    "roc_auc_score(ytest, ytest_pred)\n",
    "\n",
    "### confusion matrix for classification\n",
    "confusion_matrix(yhat,y).T\n",
    "# We transpose to get it in the format we saw in slides:\n",
    "# Rows: Predicted labels\n",
    "# Columns: True labels \n",
    "\n",
    "### accuracy via cross validation score\n",
    "scores = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "print(\"Scores:\", scores.round(2))\n",
    "print(\"Mean:\", scores.mean().round(2))\n",
    "print(\"Standard deviation:\", scores.std().round(2))\n",
    "\n",
    "### for multi class\n",
    "classification_report(y_test, clf4.predict(X_test)\n",
    "\n",
    "### MSE/RMSE\n",
    "mean_squared_error(ytrain, model.predict(Xtrain), squared=False)\n",
    "\n",
    "### custom scorer\n",
    "sc = make_scorer(mean_squared_error)\n",
    "\n",
    "### cross_val_score example\n",
    "# KFold cross-validated loss without shuffling\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "cv_scores = cross_val_score(LinearRegression(), Xtrain, ytrain, scoring=sc, cv=kf)\n",
    "print(f\"Average CV loss: %.3f +/- %.3f\" % (cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "### pipeline examples for two models\n",
    "# Define the different pipelines \n",
    "model1 = Pipeline([\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "model3 = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### matrix plots\n",
    "pd.plotting.scatter_matrix(my_data, alpha = 0.2, diagonal='kde')\n",
    "\n",
    "### joinplot - to see any variables need transformation\n",
    "ax = sns.jointplot(x=df3.Overall, y=np.log(df3.Value), kind='scatter')\n",
    "ax.ax_joint.set_xlabel('Overall')\n",
    "ax.ax_joint.set_ylabel('log(Value)')\n",
    "plt.show()\n",
    "# # You can also plot the joint probability distribution\n",
    "# kdeplot = sns.jointplot(data=D, x='weeks', y='weight', kind='kde', fill=True, cbar=True)\n",
    "# plt.subplots_adjust(left=0.1, right=0.8, top=0.9, bottom=0.1)\n",
    "# # get the current positions of the joint ax and the ax for the marginal x\n",
    "# pos_joint_ax = kdeplot.ax_joint.get_position()\n",
    "# pos_marg_x_ax = kdeplot.ax_marg_x.get_position()\n",
    "# # reposition the joint ax so it has the same width as the marginal x ax\n",
    "# kdeplot.ax_joint.set_position([pos_joint_ax.x0, pos_joint_ax.y0, pos_marg_x_ax.width, pos_joint_ax.height])\n",
    "# # reposition the colorbar using new x positions and y positions of the joint ax\n",
    "# kdeplot.fig.axes[-1].set_position([.83, pos_joint_ax.y0, .07, pos_joint_ax.height])\n",
    "\n",
    "### numerical features all in one\n",
    "df.hist(bins=15, figsize=(10,10))\n",
    "plt.show()\n",
    "\n",
    "### histogram with kde on top\n",
    "# Create histogram of target variable\n",
    "ax = model_data.overall.plot(kind='hist')\n",
    "model_data.overall.plot(kind='kde', ax=ax, secondary_y=True) # y axis on the right is for KDE\n",
    "ax.set_xlabel(\"Overall Score\")\n",
    "plt.show()\n",
    "\n",
    "### learning curve for overfitting or underfitting\n",
    "def plot_learning_curves(model, X, y, rs):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=rs)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train) + 1):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"Training Set\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation Set\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)   \n",
    "    plt.xlabel(\"Dataset Size\", fontsize=14) \n",
    "    plt.ylabel(\"RMSE\", fontsize=14)  \n",
    "\n",
    "### ggplot example\n",
    "# Code to construct the barchart \n",
    "(ggplot(spotify_pre, aes('playlist_subgenre', fill='track_popularity')) +\n",
    " geom_bar(position = \"fill\") +\n",
    " labs(x = \"Playlist sub-genre\", y = \"Proportion\") +\n",
    " coord_flip() +\n",
    " scale_y_continuous(expand = (0, 0))\n",
    ")\n",
    "\n",
    "(ggplot(spotify_pre, aes('playlist_genre', 'instrumentalness')) +\n",
    " geom_boxplot(aes(color = 'track_popularity')) +\n",
    " scale_y_log10() +\n",
    " labs(x = \"Playlist genre\", y = \"Instrumentalness score\")\n",
    ")\n",
    "\n",
    "### sns boxplot\n",
    "plt.figure(figsize=(15,9))\n",
    "boxplots = AUC_models[AUC_models.mean().sort_values().index]\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(boxplots), showfliers=False)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Simple models with only one numeric variable as predictor')\n",
    "plt.ylabel('AUC')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss \n",
    "import scipy.optimize as so\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### models\n",
    "LinearRegression()\n",
    "\n",
    "def linearModelPredict(b,X):\n",
    "    yp = np.dot(X,b) #Xrow: obs Xcol: matches length of b. #brow: matches Xcol features. bcol: length of labels\n",
    "\n",
    "    return yp\n",
    "\n",
    "#Note as long as X's cols match b (a 1D array) length we're good. b could be a row or a column vector.\n",
    "# the resulting yp will have shape of b.\n",
    "\n",
    "def linearModelLossRSS(b,X,y):\n",
    "    # The loss is really a function of b.  The b changes, the X and y stay fixed.\n",
    "    # Make predictions\n",
    "    predY = linearModelPredict(b,X)\n",
    "\n",
    "    # Compute residuals.  This is an array.  The dimension of res will depend on if\n",
    "    # b is 1d or 2d.  If b is 2d, predY will be 2d, and so res will be 2d due to something\n",
    "    # called \"array broadcasting\".\n",
    "    \n",
    "    res = y-predY\n",
    "    # Simply sum up the squared residuals.  This is the value of our loss.\n",
    "    residual_sum_of_squares = sum(res**2) \n",
    "    \n",
    "    # Because res is a vector, we can take the product of res with X.\n",
    "    # Since X is two dimensional because it is a design matrix, this results in a\n",
    "    # 2d array.  The gradient has three elements because there are three parameters.\n",
    "    gradient=-2*np.dot(res,X)\n",
    "\n",
    "    return (residual_sum_of_squares, gradient)\n",
    "\n",
    "def linearModelLossLAD(b,X,y):\n",
    "    # Same concept as before, different loss\n",
    "    predY = linearModelPredict(b,X)\n",
    "    res = y-predY\n",
    "    sres = np.sign(res); \n",
    "    sum_abs_dev = sum(abs(res))\n",
    "    # Note the gradients are computed using the sign of the residuals\n",
    "    grad =- (np.dot(sres,X))\n",
    "\n",
    "    return (sum_abs_dev,grad)\n",
    "\n",
    "\n",
    "def linearModelFit(X,y,lossfcn = linearModelLossRSS):\n",
    "    # Because we know b has to have the some dimension as X has columns,\n",
    "    # We can use the number of columns to determine the size of betas\n",
    "    # In this case, we use a 2d array\n",
    "    nrows,ncols = X.shape\n",
    "    betas=np.zeros((ncols,1))\n",
    "    # Optimize the loss\n",
    "    RES = so.minimize(lossfcn,betas,args=(X,y),jac=True, options={'disp': True})\n",
    "    # Obtain estimates from the optimizer\n",
    "    estimated_betas=RES.x \n",
    "    # Compute goodness of fit.\n",
    "    diff = y-np.mean(y)\n",
    "    TSS = sum(diff**2)\n",
    "    RSS,deriv = linearModelLossRSS(estimated_betas,X,y)\n",
    "    R2 = 1-RSS/TSS \n",
    "\n",
    "    return (estimated_betas,R2)\n",
    "\n",
    "### fitting and plotting\n",
    "    # Fit the data \n",
    "y = my_data.y.values\n",
    "pred_1 = my_data.pred_1.values\n",
    "N = pred_1.size\n",
    "print(N)\n",
    "X = np.c_[np.ones(N), pred_1]\n",
    "betas, R2 = linearModelFit(X,y)\n",
    "\n",
    "# Create new data\n",
    "pred_grid = np.linspace(pred_1.min(), pred_1.max(), 100)\n",
    "# Turn it into a design matrix\n",
    "Xn = np.c_[np.ones(pred_grid.size), pred_grid]\n",
    "\n",
    "# Compute predictions with the new data and estimated coefficients\n",
    "yn = linearModelPredict(betas, Xn)\n",
    "\n",
    "fig, ax = plt.subplots(dpi = 120)\n",
    "my_data.plot.scatter(x='pred_1', y='y', alpha=0.75, ax=ax)\n",
    "ax.set_xlabel('pred_1')\n",
    "ax.set_ylabel('target')\n",
    "\n",
    "ax.plot(pred_grid, yn, color = 'red')\n",
    "ax.annotate('R\\u00b2: {R2}'.format(R2=round(R2, 2)), \n",
    "            xy=(0.2, 0.8), \n",
    "            xycoords='axes fraction',\n",
    "            ha='center',\n",
    "            fontsize = 16)\n",
    "\n",
    "ax.set_title('RSS Model')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for this assignment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# from scipy.optimize import minimize\n",
    "import scipy.optimize as so\n",
    "import seaborn as sns\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentialNegLogLikelihood(lamb, y): # specific to the problem\n",
    "    likelihood = -np.sum(np.log(lamb)-lamb*y)\n",
    "    return likelihood\n",
    "\n",
    "def exponentialRegressionNegLogLikelihood(b, X, y):\n",
    "    # Here we have to be carefull since the mean of the exponential distribution is not the same $\\lambda$ parameter.\n",
    "    lamb = np.exp(-X @ b)\n",
    "    # Use exponentialNegLogLikelihood to compute the likelihood\n",
    "    neg_log_lik = exponentialNegLogLikelihood(lamb, y)\n",
    "    return neg_log_lik\n",
    "\n",
    "def Prediction(b, X):\n",
    "    yhat = np.exp(X @ b)\n",
    "    return yhat\n",
    "\n",
    "def Model_fit(X, y):\n",
    "    # Instantiate a guess for the betas, beta_start, so that the optimizer has somewhere to start\n",
    "    # Keep in mind what shape the beta_start should be. It shoud have the same number of elements as X as columns\n",
    "    nrows, ncols = X.shape\n",
    "    beta_start = np.zeros((ncols, 1))\n",
    "    # Minimize the appropriate likelihood function\n",
    "    mle = minimize(exponentialRegressionNegLogLikelihood, beta_start, args = (X, y), method = \"Powell\")\n",
    "    # Extract the maximum likelihood estimates from the optimizer.\n",
    "    betas = mle.x\n",
    "    return betas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### experimenting with threshold\n",
    "threshold = 0.1\n",
    "ytest_prob = ricelr.predict_proba(Xtest)\n",
    "yhat = ricelr.classes_[(ytest_prob[:,1] > threshold).astype(int)]\n",
    "\n",
    "### SGDClassifier\n",
    "# This classifier has the advantage of being capable of handling very large datasets efficiently.\n",
    "# Use for multiclass\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=seed) \n",
    "# AND\n",
    "clf4 = make_pipeline(StandardScaler(), SGDClassifier(loss='log', penalty=\"l2\", max_iter=2000, tol=1e-3, n_jobs=-1, random_state=seed)).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "### manual sigmoid probabilities\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "z1 = np.dot(Xtest[:,[0,1]],LOGREG_first_two.coef_.T) + LOGREG_first_two.intercept_\n",
    "sigmoid(z1)["
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### bootstrap for means\n",
    "def createBootstrapMeans(data, numboot=10000):\n",
    "    n = len(data)\n",
    "    boot_means = np.zeros(numboot)\n",
    "    np.random.seed(0)\n",
    "    for i in range(numboot):\n",
    "        d = data.sample(n, replace=True)\n",
    "        boot_means[i] = d.mean()\n",
    "    return boot_means\n",
    "\n",
    "### boostrap for coeffs\n",
    "def BootstrapCoef(data, numboot=1000):\n",
    "    n = len(data)\n",
    "    theta = np.zeros((numboot,3))    \n",
    "    for i in range(numboot):\n",
    "        d = data.sample(n, replace=True)\n",
    "        X_fit = np.c_[d.weeks,d.weeks**2,d.weeks**3]\n",
    "        regr.fit(X_fit,d.weight)\n",
    "        theta[i,:]=regr.coef_\n",
    "    return theta\n",
    "\n",
    "### bootstrap predictions\n",
    "def BootstrapPred(data, xp, numboot=1000):\n",
    "    n = len(data)\n",
    "    X_pred = np.c_[xp,xp**2,xp**3]\n",
    "    y_pred = np.zeros((numboot,X_pred.shape[0]))\n",
    "    for i in range(numboot):\n",
    "        d = data.sample(n, replace=True)\n",
    "        X_fit = np.c_[d.weeks,d.weeks**2,d.weeks**3]\n",
    "        regr.fit(X_fit,d.weight)\n",
    "        y_pred[i,:]=regr.predict(X_pred)\n",
    "    return y_pred\n",
    "\n",
    "### bootsrap mean predictions\n",
    "def BootstrapPred(MODEL, X_train, y_train, X_test, numboot=100):\n",
    "    y_pred = np.zeros(numboot)\n",
    "    for i in range(numboot):\n",
    "        X_fit = X_train.sample(Xtrain.shape[0], replace=True)\n",
    "        y_fit = y_train[X_fit.index]\n",
    "        y_pred[i]=np.exp(MODEL.fit(X_fit, y_fit).predict(X_test)).mean() # bc we are working with log(y)\n",
    "    return y_pred\n",
    "\n",
    "### week 5 - boostrap for auc scores\n",
    "def BootstrapPred(MODEL, X_train, y_train, numboot=100):\n",
    "    \n",
    "    Bootstrap_AUC = np.zeros(numboot)\n",
    "    for i in range(numboot):\n",
    "        X_tr_boot = X_train.sample(Xtrain.shape[0], replace=True)\n",
    "        y_tr_boot = y_train[X_tr_boot.index]\n",
    "\n",
    "        selected_ids = X_train.index.isin(X_tr_boot.index.values)\n",
    "\n",
    "        X_ts_boot = X_train[~selected_ids]\n",
    "        y_ts_boot = y_train[~selected_ids]\n",
    "\n",
    "        Bootstrap_AUC[i] = roc_auc_score(y_ts_boot, MODEL.fit(X_tr_boot, y_tr_boot).predict_proba(X_ts_boot)[:,1]) # record bootstrap statistic\n",
    "\n",
    "    return Bootstrap_AUC\n",
    "\n",
    "### CLT\n",
    "## Using central limit theorem, compute confidence interval\n",
    "\n",
    "stderr = np.std(data.Match) / np.sqrt(len(data.Match))\n",
    "print(\"Stderr: %.3f\" % stderr)\n",
    "\n",
    "# Area under a standard normal from -1.96 to 1.96 is about 95% (3 for 99.9% probability)\n",
    "critval = 1.96 # critical value\n",
    "\n",
    "# Confidence interval\n",
    "norm_ci = [(data.Match.mean() - critval*stderr).round(3), \n",
    "           (data.Match.mean() + critval*stderr).round(3)]\n",
    "\n",
    "print(\"Norm ci:\",norm_ci)\n",
    "\n",
    "# Stderr: 0.055\n",
    "# Norm ci: [0.7, 0.916]\n",
    "\n",
    "### t-distribution\n",
    "# Get the critical values for t at an alpha = 0.05/2 (i.e., 5% divided by 2), and 52-1 = 51 dof.\n",
    "alpha = (5/100)\n",
    "crit_val = 1-(alpha/2)\n",
    "\n",
    "# Degrees of freedom (dof) of an estimate is the number of independent\n",
    "# pieces of information that went into calculating the estimate.\n",
    "dof = n-1\n",
    "# the precise shape of the t distribution depends on dof, which is related to the sample size.\n",
    "\n",
    "# ppf: percent point function (inverse of cdf â€” percentiles).\n",
    "# It returns the value x of the variable that has a given cumulative distribution probability (cdf).\n",
    "# Thus, given the cdf(x) of a x value, ppf returns the value x itself, therefore, operating as the inverse of cdf.\n",
    "t_value = t.ppf(crit_val, df=dof) # 1st arg: critical value; 2nd arg: dof\n",
    "\n",
    "stderr = np.std(data.Match) / np.sqrt(len(data.Match))\n",
    "\n",
    "t_ci = data.Match.mean() + t_value * stderr * np.array([-1, 1])\n",
    "print('The t-based confidence interval is equal to {}'.format(t_ci.round(3)))\n",
    "\n",
    "### boostrap CI\n",
    "sns.set_style(\"darkgrid\") \n",
    "\n",
    "# Create a dataframe\n",
    "bm = pd.DataFrame(data=createBootstrapMeans(data)-data.Match.mean(), columns=['samples'])\n",
    "# Bootstrap with CL of 95%\n",
    "boot_CL = 95/100 \n",
    "p_1 = (1-boot_CL)/2\n",
    "p_2 = 1-p_1\n",
    "boot_quant = np.quantile(bm, [p_1, p_2])\n",
    "print('boot_quant:',boot_quant)\n",
    "\n",
    "# Compute confidence interval\n",
    "boot_ci = [(data.Match.mean() - boot_quant[1]).round(3), \n",
    "           (data.Match.mean() - boot_quant[0]).round(3)]\n",
    "print(\"Boot Confidence Interval:\",boot_ci)\n",
    "boot_ci2 = [(data.Match.mean() - np.abs(boot_quant[0])).round(3), # absolute values bc numbers are negative since data is centered. \n",
    "           (data.Match.mean() + np.abs(boot_quant[1])).round(3)]\n",
    "print(\"Boot Confidence Interval2:\",boot_ci2)\n",
    "\n",
    "# Plot histogram and KDE\n",
    "ax = bm.samples.plot(kind='hist')\n",
    "bm.samples.plot(kind='kde', ax=ax, secondary_y=True)\n",
    "ax.vlines(boot_quant[0], ymin = 0, ymax = 3500,colors='red')\n",
    "ax.vlines(boot_quant[1], ymin = 0, ymax = 3500,colors='purple')\n",
    "plt.show()\n",
    "\n",
    "### Example 2 CIs\n",
    "yhat_s    = np.exp(model.fit(Xtrain, ytrain).predict(Xtest)) # sample statistic\n",
    "mean_yhat_boot = BootstrapPred(model, Xtrain, ytrain, Xtest) # bootstrap statistic\n",
    "### CI with bootsrap\n",
    "# Bootstrap with CL of 99%\n",
    "boot_CL = 99/100 \n",
    "\n",
    "p_1 = (1-boot_CL)/2\n",
    "p_2 = 1-p_1\n",
    "\n",
    "boot_quant = np.quantile(mean_yhat_boot-yhat_s.mean(), [p_1, p_2])\n",
    "\n",
    "boot_ci = [((yhat_s.mean() - boot_quant[1])*1e-6).round(3), \n",
    "           ((yhat_s.mean() - boot_quant[0])*1e-6).round(3)]\n",
    "print(\"Boot confidence interval is\",boot_ci,'- in million euros.')\n",
    "bm = pd.DataFrame(data=BootstrapPred(model, Xtrain, ytrain, Xtest, 400)-yhat_s.mean(), columns=['Values'])\n",
    "ax = bm.Values.plot(kind='hist', bins=20)\n",
    "bm.Values.plot(kind='kde', ax=ax, secondary_y=True)\n",
    "plt.show()\n",
    "\n",
    "### CI with CLT\n",
    "# Calculate CI using CLT:\n",
    "\n",
    "stderr = yhat_s.std() / np.sqrt(yhat_s.shape[0])\n",
    "\n",
    "cl = 0.99\n",
    "print(ss.norm.ppf(cl))\n",
    "lower = yhat_s.mean() - ss.norm.ppf(cl) * stderr\n",
    "upper = yhat_s.mean() + ss.norm.ppf(cl) * stderr\n",
    "\n",
    "print('The confidence interval is [%.3f, %.3f] - in million euros.' % (lower*1e-6, upper*1e-6))\n",
    "\n",
    "### CI with t-distribution\n",
    "n = 30\n",
    "alpha = (1/100)\n",
    "crit_val = 1-(alpha/2)\n",
    "\n",
    "yhat_s_new = np.random.choice(yhat_s, size=n)\n",
    "dof = n-1\n",
    "t_value = t.ppf(crit_val, df=dof)\n",
    "stderr = yhat_s_new.std() / np.sqrt(n)\n",
    "lower = yhat_s_new.mean() - t_value * stderr \n",
    "upper = yhat_s_new.mean() + t_value * stderr\n",
    "\n",
    "print('The confidence interval is [%.3f, %.3f] - in million euros.' % (lower*1e-6, upper*1e-6))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "It was all about comparing models using cross validated AUCs, nothing new much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3018ec948c2f097dac52d5fcdb1a30ae92fa49919476c7ddb718ffa59f845b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
