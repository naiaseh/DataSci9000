{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week 1\n",
    "import scipy.stats as ss \n",
    "import scipy.optimize as so\n",
    "\n",
    "\n",
    "# week 2\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# week 3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# week 4\n",
    "from scipy.stats import t\n",
    "import scipy.stats as ss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# week 5\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, ShuffleSplit, LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # need these when use Classes to create custom transformers\n",
    "from plotnine import *\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# week 6\n",
    "from sklearn.linear_model import lasso_path, Ridge, Lasso, ElasticNetCV, LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# week 7\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from functools import reduce\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n",
    "# !pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "# !pip install gdown\n",
    "import gdown\n",
    "# !pip install pydotplus\n",
    "import pydotplus \n",
    "from tqdm import tqdm\n",
    "\n",
    "# week 8\n",
    "import os\n",
    "import re\n",
    "from   scipy.sparse import csr_matrix # compressed sparse row matrix \n",
    "import sklearn.feature_extraction.text as sktext # for sktext.TfidfVectorizer()\n",
    "from   sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "from   sklearn.manifold import TSNE\n",
    "# UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\n",
    "# !pip install umap-learn \n",
    "# !pip install datashader bokeh holoviews scikit-image colorcet ipywidgets\n",
    "import umap\n",
    "import umap.plot # good for plotting gigantic data\n",
    "from nltk.corpus import stopwords # for stopwords.words('english')\n",
    "import xgboost as xgb # for xgb.cv and .train\n",
    "\n",
    "# week 9\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "# !pip install yellowbrick\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rename\n",
    "my_data = my_data.rename(columns = {'x1':'pred_1', 'x2':'pred_2'})\n",
    "\n",
    "### conditional drop\n",
    "data = data.drop(data.index[(data['clv']<2200) | (data['clv']>16000)].tolist(),axis=0)\n",
    "\n",
    "### to see if data is balanced\n",
    "rice.CLASS.value_counts()\n",
    "\n",
    "### converting string to integer\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "### data types\n",
    "df.dtypes\n",
    "\n",
    "### null/NaN values\n",
    "df.isnull().any()\n",
    "\n",
    "### for both null and data types\n",
    "df.info()\n",
    "\n",
    "### fill null values\n",
    "df= df.fillna('M')\n",
    "\n",
    "### stats summary\n",
    "df.describe()\n",
    "\n",
    "### sort and invert indices\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "### to apply a function\n",
    "dataOHE['Total.Claim.Amount'] = dataOHE['Total.Claim.Amount'].apply(np.sqrt)\n",
    "dataOHE['clv'] = dataOHE['clv'].apply(np.log)\n",
    "\n",
    "### one hot encoding with pd.get_dummmies\n",
    "df = pd.get_dummies(df,columns=[\"Sex\", \"BP\",\"Cholesterol\"])\n",
    "model_data = pd.get_dummies(model_data, drop_first=True) \n",
    "\n",
    "### Setting binary values\n",
    "df.loc[-df[\"Drug\"].isin(['DrugY']),\"Drug\"]=\"0\"\n",
    "df.loc[df[\"Drug\"].isin(['DrugY']),\"Drug\"]=\"1\"\n",
    "\n",
    "### splitting for test and train\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=0.25,random_state=11)\n",
    "\n",
    "### correlation - note that c is of type pandas dataframe\n",
    "c = df3.corr()\n",
    "\n",
    "### for sorting\n",
    "df.sort_values(ascending=True)\n",
    "\n",
    "### example of indexing with multiple conditions\n",
    "dataLE.loc[(dataLE['work_rate_att'] == 0) | (dataLE['work_rate_att'] == 1)]\n",
    "\n",
    "### replacing a word\n",
    "netflix_data['full_description'].replace('Sci-Fi', 'Sci_Fi', inplace = True, regex = True)\n",
    "\n",
    "### example usage LabelEncoder()\n",
    "dataLE = data.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "dataLE['work_rate_att']= label_encoder.fit_transform(dataLE['work_rate_att'])\n",
    "print(dataLE.loc[dataLE['work_rate_att'] == 0].work_rate_att.count())\n",
    "print(dataLE.loc[dataLE['work_rate_att'] == 1].work_rate_att.count())\n",
    "print(dataLE.loc[dataLE['work_rate_att'] == 2].work_rate_att.count())\n",
    "print(label_encoder.classes_)\n",
    "dataLE.head()\n",
    "\n",
    "### example usage OneHotEncoder()\n",
    "# Turn category into numeric variables\n",
    "# One-Hot Encoding with sklearn OneHotEncoder()\n",
    "dataOHE = data.copy()\n",
    "ohe = OneHotEncoder()\n",
    "#reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object \n",
    "X = ohe.fit_transform(dataOHE.work_rate_att.values.reshape(-1,1)).toarray()\n",
    "#To add this back into the original dataframe \n",
    "dfOneHot = pd.DataFrame(X, columns = [\"work_rate_att_\"+str(int(i)) for i in range(3)]) \n",
    "dataOHE = pd.concat([dataOHE, dfOneHot], axis=1)\n",
    "#droping the cwork_rate_att column \n",
    "dataOHE = dataOHE.drop(['work_rate_att'], axis=1)\n",
    "print(dataOHE[(dataOHE['work_rate_att_0'] == 1.0)].work_rate_att_0.count())\n",
    "print(dataOHE[(dataOHE['work_rate_att_1'] == 1.0)].work_rate_att_1.count())\n",
    "print(dataOHE[(dataOHE['work_rate_att_2'] == 1.0)].work_rate_att_2.count())\n",
    "\n",
    "### example 2 usage OneHotEncoder()\n",
    "ohe = preprocessing.OneHotEncoder(drop='first')\n",
    "dataOHE = Data.copy()\n",
    "categorical_cols = Data.select_dtypes(include='object').columns\n",
    "for col in categorical_cols:\n",
    "    n = dataOHE[col].nunique()\n",
    "    #reshape the 1-D array to 2-D as fit_transform expects 2-D and finally fit the object \n",
    "    dummy = ohe.fit_transform(dataOHE[col].values.reshape(-1,1)).toarray()\n",
    "    # To add this back into the original dataframe \n",
    "    dfOneHot = pd.DataFrame(dummy, columns = [col+str(int(i)) for i in range(n-1)]) \n",
    "    dataOHE = pd.concat([dataOHE.reset_index(drop=True), dfOneHot.reset_index(drop=True)], axis=1)\n",
    "    #droping the cwork_rate_att column \n",
    "    dataOHE = dataOHE.drop([col], axis=1)\n",
    "dataOHE.head(3)\n",
    "\n",
    "### standard scaler\n",
    "scaler = sk.preprocessing.StandardScaler(with_mean=True,\n",
    "                                         with_std=True)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "### data string splitting and labelling based on game results\n",
    "df[[\"Home_g\",\"Away_g\"]] = df[\"result_full\"].str.split(\"-\",expand=True).astype(int)\n",
    "df[\"target\"] = df.apply(lambda x: 0 if (x[\"Home_g\"]>x[\"Away_g\"]) else (1 if (x[\"Away_g\"]>x[\"Home_g\"]) else 2),axis=1)\n",
    "df[\"Goal_difference\"] = np.abs(df[\"Home_g\"]-df[\"Away_g\"])\n",
    "df_sorted = df.sort_values(\"Goal_difference\",ascending=False).head(1)\n",
    "home_team = df_sorted[\"home_team\"].values[0]\n",
    "away_team = df_sorted[\"away_team\"].values[0]\n",
    "game_result = df_sorted[\"result_full\"].values[0]\n",
    "print(f\"The game with greatest goal difference is {home_team} - {away_team} with a result of {game_result}.\")\n",
    "df.drop(columns = [\"home_team\",\"away_team\",\"Home_g\",\"Away_g\",\"result_full\",\"Goal_difference\"],inplace=True)\n",
    "\n",
    "### sorted indices\n",
    "Indices_sorted = np.argsort(var_ratios)[::-1]\n",
    "\n",
    "### select dtypes \n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "### PolyNomialFeatures Example\n",
    "X = X.values.reshape(-1,X.shape[1])\n",
    "poly = PolynomialFeatures(degree=2,include_bias=0)\n",
    "Xpoly = poly.fit_transform(X)\n",
    "featureNamesPoly = poly.get_feature_names_out()\n",
    "Xpoly_df = pd.DataFrame(Xpoly,columns=featureNamesPoly)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### baseline accuracy\n",
    "A  = rice.CLASS[rice['CLASS']=='Osmancik'].count() # majority class\n",
    "B = rice.CLASS[rice['CLASS']=='Cammeo'].count()\n",
    "\n",
    "baselineacc = A/(A+B) # the classifier you train must beat this number else no point in training one\n",
    "\n",
    "print('Baseline Accuracy: '+str((baselineacc*100).round(1))+'%')\n",
    "\n",
    "### label-base accuracy, precision, specificty and recall\n",
    "def compute_performance(yhat, y, classes):\n",
    "    # First, get tp, tn, fp, fn\n",
    "    tp = sum(np.logical_and(yhat == classes[1], y == classes[1]))\n",
    "    tn = sum(np.logical_and(yhat == classes[0], y == classes[0]))\n",
    "    fp = sum(np.logical_and(yhat == classes[1], y == classes[0]))\n",
    "    fn = sum(np.logical_and(yhat == classes[0], y == classes[1]))\n",
    "\n",
    "    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Precision\n",
    "    # \"Of the ones I labeled +, how many are actually +?\"\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    # Recall\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    recall = tp / (tp + fn)    \n",
    "    \n",
    "    # Sensitivity\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    sensitivity = recall\n",
    "    \n",
    "    # Specificity\n",
    "    # \"Of all the - in the data, how many do I correctly label?\"\n",
    "    specificity = tn / (fp + tn)\n",
    "    \n",
    "    # Print results\n",
    "    \n",
    "    print(\"Accuracy:\",round(acc,3),\"Recall:\",round(recall,3),\"Precision:\",round(precision,3),\n",
    "          \"Sensitivity:\",round(sensitivity,3),\"Specificity:\",round(specificity,3))\n",
    "\n",
    "compute_performance(ytest_hat, ytest, ricelr.classes_)\n",
    "\n",
    "### ROC ranking-based metric for classification\n",
    "fpr, tpr, _ = roc_curve(ytest, ytest_prob[:,1], pos_label=ricelr.classes_[1]) # 2nd arg: ranking score, 3rd arg: \"Osmancik\"\n",
    "ax =sns.lineplot(x=fpr,y=tpr)\n",
    "plt.plot([0,1], [0,1], \"r--\")\n",
    "ax.set_xlabel(\"FP Rate\")\n",
    "ax.set_ylabel(\"TP Rate\")\n",
    "\n",
    "### auc\n",
    "auc(fpr,tpr)\n",
    "\n",
    "### auc score from ytrue and ypred\n",
    "roc_auc_score(ytest, ytest_pred)\n",
    "\n",
    "### confusion matrix for classification\n",
    "confusion_matrix(yhat,y).T\n",
    "# We transpose to get it in the format we saw in slides:\n",
    "# Rows: Predicted labels\n",
    "# Columns: True labels \n",
    "\n",
    "### confusion matrix example 2\n",
    "# Calculate confusion matrix\n",
    "confusion_matrix_rf = confusion_matrix(y_true = bankloan_test_noWoE['Default'], y_pred = rf_pred_class_test)\n",
    "# Turn matrix to percentages\n",
    "confusion_matrix_rf = confusion_matrix_rf.astype('float') / confusion_matrix_rf.sum(axis=1)[:, np.newaxis]\n",
    "# Turn to dataframe\n",
    "df_cm = pd.DataFrame(confusion_matrix_rf, index=['Good', 'Bad'], columns=['Good', 'Bad'])\n",
    "# Parameters of the image\n",
    "figsize = (10,7)\n",
    "fontsize=14\n",
    "# Create image\n",
    "fig = plt.figure(figsize=figsize)\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
    "# Make it nicer\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0,  ha='right', fontsize=fontsize)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "# Add labels\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "# Plot!\n",
    "plt.show()\n",
    "\n",
    "### confusion matrix example 3\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=clf.predict(X_test))/len(y_test)  # \"/len(y_test)\" is to get them in percentage\n",
    "print(cm.round(2))\n",
    "# or\n",
    "ConfusionMatrixDisplay(cm, display_labels = ['Home wins', 'Away wins', 'Tie']).plot()\n",
    "plt.show()\n",
    "\n",
    "### accuracy via cross validation score\n",
    "scores = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "print(\"Scores:\", scores.round(2))\n",
    "print(\"Mean:\", scores.mean().round(2))\n",
    "print(\"Standard deviation:\", scores.std().round(2))\n",
    "\n",
    "### for multi class\n",
    "classification_report(y_test, clf4.predict(X_test)\n",
    "\n",
    "### MSE/RMSE\n",
    "mean_squared_error(ytrain, model.predict(Xtrain), squared=False)\n",
    "\n",
    "### custom scorer\n",
    "sc = make_scorer(mean_squared_error)\n",
    "\n",
    "### cross_val_score example\n",
    "# KFold cross-validated loss without shuffling\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "cv_scores = cross_val_score(LinearRegression(), Xtrain, ytrain, scoring=sc, cv=kf)\n",
    "print(f\"Average CV loss: %.3f +/- %.3f\" % (cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "### pipeline examples for two models\n",
    "# Define the different pipelines \n",
    "model1 = Pipeline([\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "model3 = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "\n",
    "### variable importance with lasso path\n",
    "# either set eps and n_alphas OR have a list of alphas\n",
    "eps = 5e-3 # The smaller eps, the longer the path  \n",
    "lambda_lasso, coefs_lasso, dual_gaps, n_iters = skl.lasso_path(X, D.y, eps=eps, n_alphas=100, alphas= None) \n",
    "print(f'minimum regularization parameter : %.3f'% np.amin(lambda_lasso))\n",
    "print(f'maximum regularization parameter : %.3f' % np.amax(lambda_lasso))\n",
    "colors = ['b', 'r', 'g', 'c', 'k','c']\n",
    "# color=[\"#\"+''.join([random.choice('0123456789ABCDEF') for i in range(6)])\n",
    "#        for j in range(NumCoef)]\n",
    "# Legend = []\n",
    "neg_log_lambda = -np.log(lambda_lasso)\n",
    "for i in range(6):\n",
    "    l1 = plt.plot(neg_log_lambda, coefs_lasso[i,], c=colors[i])\n",
    "    # Legend.append(str(i+1))\n",
    "plt.xlabel('Neg. Log. Lambda')\n",
    "plt.ylabel('Coef.')\n",
    "plt.legend(['Coef. 1', 'Coef. 2', 'Coef. 3',\n",
    "           'Coef. 4', 'Coef. 5', 'Coef. 6'])\n",
    "plt.show()\n",
    "\n",
    "### accuracy\n",
    "accuracy1 = accuracy_score(y_true = y_test, y_pred = clf.predict(X_test)) \n",
    "\n",
    "### silhouette score\n",
    "silhouette_score(X, synth_df['cluster_label_scaled']) # where synth_df['cluster_label_scaled'] is the predicted labels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### matrix plots\n",
    "pd.plotting.scatter_matrix(my_data, alpha = 0.2, diagonal='kde')\n",
    "\n",
    "### joinplot - to see any variables need transformation\n",
    "ax = sns.jointplot(x=df3.Overall, y=np.log(df3.Value), kind='scatter')\n",
    "ax.ax_joint.set_xlabel('Overall')\n",
    "ax.ax_joint.set_ylabel('log(Value)')\n",
    "plt.show()\n",
    "# # You can also plot the joint probability distribution\n",
    "# kdeplot = sns.jointplot(data=D, x='weeks', y='weight', kind='kde', fill=True, cbar=True)\n",
    "# plt.subplots_adjust(left=0.1, right=0.8, top=0.9, bottom=0.1)\n",
    "# # get the current positions of the joint ax and the ax for the marginal x\n",
    "# pos_joint_ax = kdeplot.ax_joint.get_position()\n",
    "# pos_marg_x_ax = kdeplot.ax_marg_x.get_position()\n",
    "# # reposition the joint ax so it has the same width as the marginal x ax\n",
    "# kdeplot.ax_joint.set_position([pos_joint_ax.x0, pos_joint_ax.y0, pos_marg_x_ax.width, pos_joint_ax.height])\n",
    "# # reposition the colorbar using new x positions and y positions of the joint ax\n",
    "# kdeplot.fig.axes[-1].set_position([.83, pos_joint_ax.y0, .07, pos_joint_ax.height])\n",
    "\n",
    "### numerical features all in one\n",
    "df.hist(bins=15, figsize=(10,10))\n",
    "plt.show()\n",
    "\n",
    "### histogram with kde on top\n",
    "# Create histogram of target variable\n",
    "ax = model_data.overall.plot(kind='hist')\n",
    "model_data.overall.plot(kind='kde', ax=ax, secondary_y=True) # y axis on the right is for KDE\n",
    "ax.set_xlabel(\"Overall Score\")\n",
    "plt.show()\n",
    "\n",
    "### learning curve for overfitting or underfitting\n",
    "def plot_learning_curves(model, X, y, rs):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=rs)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train) + 1):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"Training Set\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation Set\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)   \n",
    "    plt.xlabel(\"Dataset Size\", fontsize=14) \n",
    "    plt.ylabel(\"RMSE\", fontsize=14)  \n",
    "\n",
    "### ggplot example\n",
    "# Code to construct the barchart \n",
    "(ggplot(spotify_pre, aes('playlist_subgenre', fill='track_popularity')) +\n",
    " geom_bar(position = \"fill\") +\n",
    " labs(x = \"Playlist sub-genre\", y = \"Proportion\") +\n",
    " coord_flip() +\n",
    " scale_y_continuous(expand = (0, 0))\n",
    ")\n",
    "\n",
    "(ggplot(spotify_pre, aes('playlist_genre', 'instrumentalness')) +\n",
    " geom_boxplot(aes(color = 'track_popularity')) +\n",
    " scale_y_log10() +\n",
    " labs(x = \"Playlist genre\", y = \"Instrumentalness score\")\n",
    ")\n",
    "\n",
    "### sns boxplot\n",
    "plt.figure(figsize=(15,9))\n",
    "boxplots = AUC_models[AUC_models.mean().sort_values().index]\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(boxplots), showfliers=False)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Simple models with only one numeric variable as predictor')\n",
    "plt.ylabel('AUC')\n",
    "\n",
    "### importances with gini for RF\n",
    "# Plot variable importance\n",
    "indices = np.argsort(importances)[::-1] \n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - Random Forest\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[bankloan_rf.feature_names_in_[i] for i in indices], x=importances[indices], label=\"Total\", color=\"b\")\n",
    "# or\n",
    "# sns.barplot(y=[bankloan_train_noWoE.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Attribute\", xlabel=\"Attribute Importance (Entropy)\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "### importances with XGBoost\n",
    "# Plot variable importance\n",
    "importances = XGB_Bankloan.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGBoosting\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[bankloan_train_noWoE.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Gini)\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "### importances with XGBoost method 2\n",
    "xgboost.plot_importance(final_XGB, importance_type=\"gain\")\n",
    "plt.title('XGboost Variable Importance')\n",
    "plt.show()\n",
    "\n",
    "### Plotting explained variance with number of concepts [ / 4 marks]\n",
    "plt.plot(np.arange(1, 25), np.cumsum(PCA_variances))\n",
    "plt.xlabel('Number of concepts')\n",
    "plt.ylabel('Explained variance %')\n",
    "\n",
    "### KElbow\n",
    "# Initialize the object\n",
    "visualizer = KElbowVisualizer(KClusterer, # Cluster model with any parameters you need\n",
    "                              k=(2,12),   # Number of clusters to test (2 to 12 in this case)\n",
    "                              locate_elbow=True, # Locate the elbow? Default is true.\n",
    "                              timings=True # Plot the timings to train?\n",
    "                             )   \n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "### 3D plot\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = Axes3D(fig, elev=60, azim=134, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "\n",
    "colors = {1:'tab:blue', 2:'tab:orange', 3:'tab:green', 4:'tab:red', 5:'tab:purple'}\n",
    "ax.scatter(TSNE_embedding[:, 0], TSNE_embedding[:, 1], TSNE_embedding[:, 2], alpha=0.8, c=df1[label].map(colors))\n",
    "ax.view_init(azim=-50, elev=10)\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "\n",
    "### Silhoutte plot (knife plot)\n",
    "# We can plot a silhouette plot for k-means using yellowbricks\n",
    "visualizer = SilhouetteVisualizer(kmeans_pipe[1]) # Get the kmeans model\n",
    "visualizer.fit(kmeans_pipe[0].transform(X)) # Pass the scaled data\n",
    "visualizer.show() # show the plot\n",
    "\n",
    "### OR manually\n",
    "# 16pts\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 1 columns\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    ax1.set_xlim([0, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=9)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is : %.4f\"  % silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    \n",
    "    # Iterate over the clusters\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"Silhouette plot for the various clusters (n_clusters=%i).\" % (n_clusters))\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss \n",
    "import scipy.optimize as so\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### models\n",
    "LinearRegression()\n",
    "\n",
    "def linearModelPredict(b,X):\n",
    "    yp = np.dot(X,b) #Xrow: obs Xcol: matches length of b. #brow: matches Xcol features. bcol: length of labels\n",
    "\n",
    "    return yp\n",
    "\n",
    "#Note as long as X's cols match b (a 1D array) length we're good. b could be a row or a column vector.\n",
    "# the resulting yp will have shape of b.\n",
    "\n",
    "def linearModelLossRSS(b,X,y):\n",
    "    # The loss is really a function of b.  The b changes, the X and y stay fixed.\n",
    "    # Make predictions\n",
    "    predY = linearModelPredict(b,X)\n",
    "\n",
    "    # Compute residuals.  This is an array.  The dimension of res will depend on if\n",
    "    # b is 1d or 2d.  If b is 2d, predY will be 2d, and so res will be 2d due to something\n",
    "    # called \"array broadcasting\".\n",
    "    \n",
    "    res = y-predY\n",
    "    # Simply sum up the squared residuals.  This is the value of our loss.\n",
    "    residual_sum_of_squares = sum(res**2) \n",
    "    \n",
    "    # Because res is a vector, we can take the product of res with X.\n",
    "    # Since X is two dimensional because it is a design matrix, this results in a\n",
    "    # 2d array.  The gradient has three elements because there are three parameters.\n",
    "    gradient=-2*np.dot(res,X)\n",
    "\n",
    "    return (residual_sum_of_squares, gradient)\n",
    "\n",
    "def linearModelLossLAD(b,X,y):\n",
    "    # Same concept as before, different loss\n",
    "    predY = linearModelPredict(b,X)\n",
    "    res = y-predY\n",
    "    sres = np.sign(res); \n",
    "    sum_abs_dev = sum(abs(res))\n",
    "    # Note the gradients are computed using the sign of the residuals\n",
    "    grad =- (np.dot(sres,X))\n",
    "\n",
    "    return (sum_abs_dev,grad)\n",
    "\n",
    "\n",
    "def linearModelFit(X,y,lossfcn = linearModelLossRSS):\n",
    "    # Because we know b has to have the some dimension as X has columns,\n",
    "    # We can use the number of columns to determine the size of betas\n",
    "    # In this case, we use a 2d array\n",
    "    nrows,ncols = X.shape\n",
    "    betas=np.zeros((ncols,1))\n",
    "    # Optimize the loss\n",
    "    RES = so.minimize(lossfcn,betas,args=(X,y),jac=True, options={'disp': True})\n",
    "    # Obtain estimates from the optimizer\n",
    "    estimated_betas=RES.x \n",
    "    # Compute goodness of fit.\n",
    "    diff = y-np.mean(y)\n",
    "    TSS = sum(diff**2)\n",
    "    RSS,deriv = linearModelLossRSS(estimated_betas,X,y)\n",
    "    R2 = 1-RSS/TSS \n",
    "\n",
    "    return (estimated_betas,R2)\n",
    "\n",
    "### fitting and plotting\n",
    "    # Fit the data \n",
    "y = my_data.y.values\n",
    "pred_1 = my_data.pred_1.values\n",
    "N = pred_1.size\n",
    "print(N)\n",
    "X = np.c_[np.ones(N), pred_1]\n",
    "betas, R2 = linearModelFit(X,y)\n",
    "\n",
    "# Create new data\n",
    "pred_grid = np.linspace(pred_1.min(), pred_1.max(), 100)\n",
    "# Turn it into a design matrix\n",
    "Xn = np.c_[np.ones(pred_grid.size), pred_grid]\n",
    "\n",
    "# Compute predictions with the new data and estimated coefficients\n",
    "yn = linearModelPredict(betas, Xn)\n",
    "\n",
    "fig, ax = plt.subplots(dpi = 120)\n",
    "my_data.plot.scatter(x='pred_1', y='y', alpha=0.75, ax=ax)\n",
    "ax.set_xlabel('pred_1')\n",
    "ax.set_ylabel('target')\n",
    "\n",
    "ax.plot(pred_grid, yn, color = 'red')\n",
    "ax.annotate('R\\u00b2: {R2}'.format(R2=round(R2, 2)), \n",
    "            xy=(0.2, 0.8), \n",
    "            xycoords='axes fraction',\n",
    "            ha='center',\n",
    "            fontsize = 16)\n",
    "\n",
    "ax.set_title('RSS Model')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for this assignment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# from scipy.optimize import minimize\n",
    "import scipy.optimize as so\n",
    "import seaborn as sns\n",
    "# from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentialNegLogLikelihood(lamb, y): # specific to the problem\n",
    "    likelihood = -np.sum(np.log(lamb)-lamb*y)\n",
    "    return likelihood\n",
    "\n",
    "def exponentialRegressionNegLogLikelihood(b, X, y):\n",
    "    # Here we have to be carefull since the mean of the exponential distribution is not the same $\\lambda$ parameter.\n",
    "    lamb = np.exp(-X @ b)\n",
    "    # Use exponentialNegLogLikelihood to compute the likelihood\n",
    "    neg_log_lik = exponentialNegLogLikelihood(lamb, y)\n",
    "    return neg_log_lik\n",
    "\n",
    "def Prediction(b, X):\n",
    "    yhat = np.exp(X @ b)\n",
    "    return yhat\n",
    "\n",
    "def Model_fit(X, y):\n",
    "    # Instantiate a guess for the betas, beta_start, so that the optimizer has somewhere to start\n",
    "    # Keep in mind what shape the beta_start should be. It shoud have the same number of elements as X as columns\n",
    "    nrows, ncols = X.shape\n",
    "    beta_start = np.zeros((ncols, 1))\n",
    "    # Minimize the appropriate likelihood function\n",
    "    mle = minimize(exponentialRegressionNegLogLikelihood, beta_start, args = (X, y), method = \"Powell\")\n",
    "    # Extract the maximum likelihood estimates from the optimizer.\n",
    "    betas = mle.x\n",
    "    return betas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### experimenting with threshold\n",
    "threshold = 0.1\n",
    "ytest_prob = ricelr.predict_proba(Xtest)\n",
    "yhat = ricelr.classes_[(ytest_prob[:,1] > threshold).astype(int)]\n",
    "\n",
    "### SGDClassifier\n",
    "# This classifier has the advantage of being capable of handling very large datasets efficiently.\n",
    "# Use for multiclass\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=seed) \n",
    "# AND\n",
    "clf4 = make_pipeline(StandardScaler(), SGDClassifier(loss='log', penalty=\"l2\", max_iter=2000, tol=1e-3, n_jobs=-1, random_state=seed)).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "### manual sigmoid probabilities\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "z1 = np.dot(Xtest[:,[0,1]],LOGREG_first_two.coef_.T) + LOGREG_first_two.intercept_\n",
    "sigmoid(z1)["
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### bootstrap for means\n",
    "def createBootstrapMeans(data, numboot=10000):\n",
    "    n = len(data)\n",
    "    boot_means = np.zeros(numboot)\n",
    "    np.random.seed(0)\n",
    "    for i in range(numboot):\n",
    "        d = data.sample(n, replace=True)\n",
    "        boot_means[i] = d.mean()\n",
    "    return boot_means\n",
    "\n",
    "### boostrap for coeffs\n",
    "def BootstrapCoef(data, numboot=1000):\n",
    "    n = len(data)\n",
    "    theta = np.zeros((numboot,3))    \n",
    "    for i in range(numboot):\n",
    "        d = data.sample(n, replace=True)\n",
    "        X_fit = np.c_[d.weeks,d.weeks**2,d.weeks**3]\n",
    "        regr.fit(X_fit,d.weight)\n",
    "        theta[i,:]=regr.coef_\n",
    "    return theta\n",
    "\n",
    "### bootstrap predictions\n",
    "def BootstrapPred(data, xp, numboot=1000):\n",
    "    n = len(data)\n",
    "    X_pred = np.c_[xp,xp**2,xp**3]\n",
    "    y_pred = np.zeros((numboot,X_pred.shape[0]))\n",
    "    for i in range(numboot):\n",
    "        d = data.sample(n, replace=True)\n",
    "        X_fit = np.c_[d.weeks,d.weeks**2,d.weeks**3]\n",
    "        regr.fit(X_fit,d.weight)\n",
    "        y_pred[i,:]=regr.predict(X_pred)\n",
    "    return y_pred\n",
    "\n",
    "### bootsrap mean predictions\n",
    "def BootstrapPred(MODEL, X_train, y_train, X_test, numboot=100):\n",
    "    y_pred = np.zeros(numboot)\n",
    "    for i in range(numboot):\n",
    "        X_fit = X_train.sample(Xtrain.shape[0], replace=True)\n",
    "        y_fit = y_train[X_fit.index]\n",
    "        y_pred[i]=np.exp(MODEL.fit(X_fit, y_fit).predict(X_test)).mean() # bc we are working with log(y)\n",
    "    return y_pred\n",
    "\n",
    "### week 5 - boostrap for auc scores\n",
    "def BootstrapPred(MODEL, X_train, y_train, numboot=100):\n",
    "    \n",
    "    Bootstrap_AUC = np.zeros(numboot)\n",
    "    for i in range(numboot):\n",
    "        X_tr_boot = X_train.sample(Xtrain.shape[0], replace=True)\n",
    "        y_tr_boot = y_train[X_tr_boot.index]\n",
    "\n",
    "        selected_ids = X_train.index.isin(X_tr_boot.index.values)\n",
    "\n",
    "        X_ts_boot = X_train[~selected_ids]\n",
    "        y_ts_boot = y_train[~selected_ids]\n",
    "\n",
    "        Bootstrap_AUC[i] = roc_auc_score(y_ts_boot, MODEL.fit(X_tr_boot, y_tr_boot).predict_proba(X_ts_boot)[:,1]) # record bootstrap statistic\n",
    "\n",
    "    return Bootstrap_AUC\n",
    "\n",
    "### CLT\n",
    "## Using central limit theorem, compute confidence interval\n",
    "\n",
    "stderr = np.std(data.Match) / np.sqrt(len(data.Match))\n",
    "print(\"Stderr: %.3f\" % stderr)\n",
    "\n",
    "# Area under a standard normal from -1.96 to 1.96 is about 95% (3 for 99.9% probability)\n",
    "critval = 1.96 # critical value\n",
    "\n",
    "# Confidence interval\n",
    "norm_ci = [(data.Match.mean() - critval*stderr).round(3), \n",
    "           (data.Match.mean() + critval*stderr).round(3)]\n",
    "\n",
    "print(\"Norm ci:\",norm_ci)\n",
    "\n",
    "# Stderr: 0.055\n",
    "# Norm ci: [0.7, 0.916]\n",
    "\n",
    "### t-distribution\n",
    "# Get the critical values for t at an alpha = 0.05/2 (i.e., 5% divided by 2), and 52-1 = 51 dof.\n",
    "alpha = (5/100)\n",
    "crit_val = 1-(alpha/2)\n",
    "\n",
    "# Degrees of freedom (dof) of an estimate is the number of independent\n",
    "# pieces of information that went into calculating the estimate.\n",
    "dof = n-1\n",
    "# the precise shape of the t distribution depends on dof, which is related to the sample size.\n",
    "\n",
    "# ppf: percent point function (inverse of cdf — percentiles).\n",
    "# It returns the value x of the variable that has a given cumulative distribution probability (cdf).\n",
    "# Thus, given the cdf(x) of a x value, ppf returns the value x itself, therefore, operating as the inverse of cdf.\n",
    "t_value = t.ppf(crit_val, df=dof) # 1st arg: critical value; 2nd arg: dof\n",
    "\n",
    "stderr = np.std(data.Match) / np.sqrt(len(data.Match))\n",
    "\n",
    "t_ci = data.Match.mean() + t_value * stderr * np.array([-1, 1])\n",
    "print('The t-based confidence interval is equal to {}'.format(t_ci.round(3)))\n",
    "\n",
    "### boostrap CI\n",
    "sns.set_style(\"darkgrid\") \n",
    "\n",
    "# Create a dataframe\n",
    "bm = pd.DataFrame(data=createBootstrapMeans(data)-data.Match.mean(), columns=['samples'])\n",
    "# Bootstrap with CL of 95%\n",
    "boot_CL = 95/100 \n",
    "p_1 = (1-boot_CL)/2\n",
    "p_2 = 1-p_1\n",
    "boot_quant = np.quantile(bm, [p_1, p_2])\n",
    "print('boot_quant:',boot_quant)\n",
    "\n",
    "# Compute confidence interval\n",
    "boot_ci = [(data.Match.mean() - boot_quant[1]).round(3), \n",
    "           (data.Match.mean() - boot_quant[0]).round(3)]\n",
    "print(\"Boot Confidence Interval:\",boot_ci)\n",
    "boot_ci2 = [(data.Match.mean() - np.abs(boot_quant[0])).round(3), # absolute values bc numbers are negative since data is centered. \n",
    "           (data.Match.mean() + np.abs(boot_quant[1])).round(3)]\n",
    "print(\"Boot Confidence Interval2:\",boot_ci2)\n",
    "\n",
    "# Plot histogram and KDE\n",
    "ax = bm.samples.plot(kind='hist')\n",
    "bm.samples.plot(kind='kde', ax=ax, secondary_y=True)\n",
    "ax.vlines(boot_quant[0], ymin = 0, ymax = 3500,colors='red')\n",
    "ax.vlines(boot_quant[1], ymin = 0, ymax = 3500,colors='purple')\n",
    "plt.show()\n",
    "\n",
    "### Example 2 CIs\n",
    "yhat_s    = np.exp(model.fit(Xtrain, ytrain).predict(Xtest)) # sample statistic\n",
    "mean_yhat_boot = BootstrapPred(model, Xtrain, ytrain, Xtest) # bootstrap statistic\n",
    "### CI with bootsrap\n",
    "# Bootstrap with CL of 99%\n",
    "boot_CL = 99/100 \n",
    "\n",
    "p_1 = (1-boot_CL)/2\n",
    "p_2 = 1-p_1\n",
    "\n",
    "boot_quant = np.quantile(mean_yhat_boot-yhat_s.mean(), [p_1, p_2])\n",
    "\n",
    "boot_ci = [((yhat_s.mean() - boot_quant[1])*1e-6).round(3), \n",
    "           ((yhat_s.mean() - boot_quant[0])*1e-6).round(3)]\n",
    "print(\"Boot confidence interval is\",boot_ci,'- in million euros.')\n",
    "bm = pd.DataFrame(data=BootstrapPred(model, Xtrain, ytrain, Xtest, 400)-yhat_s.mean(), columns=['Values'])\n",
    "ax = bm.Values.plot(kind='hist', bins=20)\n",
    "bm.Values.plot(kind='kde', ax=ax, secondary_y=True)\n",
    "plt.show()\n",
    "\n",
    "### CI with CLT\n",
    "# Calculate CI using CLT:\n",
    "\n",
    "stderr = yhat_s.std() / np.sqrt(yhat_s.shape[0])\n",
    "\n",
    "cl = 0.99\n",
    "print(ss.norm.ppf(cl))\n",
    "lower = yhat_s.mean() - ss.norm.ppf(cl) * stderr\n",
    "upper = yhat_s.mean() + ss.norm.ppf(cl) * stderr\n",
    "\n",
    "print('The confidence interval is [%.3f, %.3f] - in million euros.' % (lower*1e-6, upper*1e-6))\n",
    "\n",
    "### CI with t-distribution\n",
    "n = 30\n",
    "alpha = (1/100)\n",
    "crit_val = 1-(alpha/2)\n",
    "\n",
    "yhat_s_new = np.random.choice(yhat_s, size=n)\n",
    "dof = n-1\n",
    "t_value = t.ppf(crit_val, df=dof)\n",
    "stderr = yhat_s_new.std() / np.sqrt(n)\n",
    "lower = yhat_s_new.mean() - t_value * stderr \n",
    "upper = yhat_s_new.mean() + t_value * stderr\n",
    "\n",
    "print('The confidence interval is [%.3f, %.3f] - in million euros.' % (lower*1e-6, upper*1e-6))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "It was all about comparing models using cross validated AUCs, nothing new much."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ridge\n",
    "ridge = skl.Ridge(alpha=1.0, fit_intercept=False)\n",
    "ridge.fit(X,D.y)\n",
    "ypp=ridge.predict(X)\n",
    "plt.plot(x,D.y,'k.',x,yp,'r-',x,ypp,'b-')\n",
    "plt.show()\n",
    "# for manual extraction of min Lambda\n",
    "Ind = np.where(mse == np.min(mse)) # where mse is -cross_val_scores\n",
    "print(lam[Ind]) # alpha\n",
    "\n",
    "### lasso \n",
    "las = skl.Lasso(alpha=0.005, fit_intercept=True)\n",
    "las.fit(X,D.y)\n",
    "yl=las.predict(X)\n",
    "\n",
    "### ElasticNetCV\n",
    "ElasticNet = skl.ElasticNetCV(eps=eps, n_alphas=10, \n",
    "                              l1_ratio=[.1, .5, .7, .9, .95, .99, 1],\n",
    "                              alphas=None,\n",
    "                              fit_intercept=True, max_iter=1000,\n",
    "                              tol=0.001, cv=3, \n",
    "                              n_jobs=2,\n",
    "                              random_state=20220214)\n",
    "ElasticNet.fit(X, D.y)\n",
    "# Print best coefficients\n",
    "print('Best fit: alpha=%.3f, l1_ratio=%.3f' % \n",
    "      (ElasticNet.alpha_, ElasticNet.l1_ratio_))\n",
    "\n",
    "### sequential feature selector\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=20)\n",
    "sfs.fit(Xtrain,ytrain)\n",
    "sfs.get_support(indices=True) # if true, it will output integer indices instead of boolean array\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sklearn tree\n",
    "reg = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "### sklearn random forest\n",
    "#Define the classifier\n",
    "bankloan_rf = RandomForestClassifier(n_estimators=1000, # Number of trees to train\n",
    "                       criterion='entropy', # How to train the trees. Also supports entropy.\n",
    "                       max_depth=None, # Max depth of the trees. Not necessary to change.\n",
    "                       min_samples_split=2, # Minimum samples to create a split.\n",
    "                       min_samples_leaf=0.001, # Minimum samples in a leaf. Accepts fractions for %. This is 0.1% of sample.\n",
    "                       min_weight_fraction_leaf=0.0, # Same as above, but uses the class weights.\n",
    "                       max_features='auto', # Maximum number of features per split (not tree!) by default is sqrt(vars)\n",
    "                       max_leaf_nodes=None, # Maximum number of nodes.\n",
    "                       min_impurity_decrease=0.0001, # Minimum impurity decrease. This is 10^-3.\n",
    "                       bootstrap=True, # If sample with repetition. For large samples (>100.000) set to false. If False, the whole dataset is used to build each tree.\n",
    "                       oob_score=True,  # If report accuracy with non-selected cases.\n",
    "                       n_jobs=-1, # Parallel processing. Set to -1 for all cores. Watch your RAM!!\n",
    "                       random_state=seed, # Seed\n",
    "                       verbose=1, # If to give info during training. Set to 0 for silent training.\n",
    "                       warm_start=False, # If train over previously trained tree.\n",
    "                       class_weight='balanced'\n",
    "                                    )\n",
    "\n",
    "# These are parameters to limit the growth of the tree. If you need to constrain the growth of the tree, pick one of them.\n",
    "# max_depth\n",
    "# min_samples_split\n",
    "# min_samples_leaf  \n",
    "# min_weight_fraction_leaf\n",
    "# min_impurity_decrease  # this is more agnostic to the sample size. If you really need to constrain growth, this is preferred\n",
    "bankloan_rf.oob_score_\n",
    "# can think of this as validation score \n",
    "bankloan_rf.score(X, y) # return mean accuracy \n",
    "# importances\n",
    "importances = bankloan_rf.feature_importances_\n",
    "print(bankloan_rf.feature_names_in_)\n",
    "print(importances)\n",
    "\n",
    "### read pickle files\n",
    "# Read from the file we just got.\n",
    "bankloan_data = pd.read_pickle('BankloanClean.pkl')\n",
    "\n",
    "# Drop a categorical variable\n",
    "bankloan_data.drop('Education', inplace = True, axis = 1)\n",
    "\n",
    "### XGBoostClassifer\n",
    "#Define the classifier.\n",
    "\n",
    "# Specify the learning task and the corresponding learning objective. The objective options are below:\n",
    "# https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters\n",
    "\n",
    "XGB_Bankloan = XGBClassifier(max_depth=3,                 # Depth of each tree\n",
    "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=100,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=-1,                    # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            scale_pos_weight=950.0/539.0, # Control the balance of positive and negative weights, useful for unbalanced classes.\n",
    "                                                          # sum(negative instances) / sum(positive instances)\n",
    "                            base_score=0.5,               # The initial prediction score of all instances, global bias. Set to average of the target rate.\n",
    "                            random_state=seed             # Seed\n",
    "                            )\n",
    "### GridsearchCV\n",
    "# Define the parameters. Play with this grid!\n",
    "param_grid = dict({'n_estimators'  : [50, 100, 150],\n",
    "                   'max_depth'     : [2, 3],\n",
    "                   'learning_rate' : [0.001, 0.01, 0.1]\n",
    "                  })\n",
    "cv_object = StratifiedKFold(n_splits=3)\n",
    "# Define grid search object.\n",
    "GridXGB = GridSearchCV(XGB_Bankloan,        # Original XGB. \n",
    "                       param_grid,          # Parameter grid\n",
    "                       cv = cv_object,      # Cross-validation object.  \n",
    "                       scoring = 'roc_auc', # How to rank outputs.\n",
    "                       n_jobs = -1,          # Parallel jobs. -1 is \"all you have\"\n",
    "                       refit = False,       # If refit at the end with the best. We'll do it manually.\n",
    "                       verbose = 1          # If to show what it is doing.\n",
    "                      )\n",
    "GridXGB.fit(val_train.iloc[:, :-1], val_train['Default'])\n",
    "GridXGB.best_params_\n",
    "\n",
    "### Question asked for correlation data frame with only the ones greater than 0.9\n",
    "PCC_final = pd.DataFrame()\n",
    "var1 = []\n",
    "var2 = []\n",
    "PCC_score = []\n",
    "for c,col in enumerate(PCC.columns): # where pcc is df.corr()\n",
    "    for r,row in enumerate(PCC.index):\n",
    "        var1.append(col)\n",
    "        var2.append(row)\n",
    "        PCC_score.append(PCC.iloc[r,c])\n",
    "print(len(var1),len(var2),len(PCC_score))\n",
    "PCC_final['Variable 1'] = var1\n",
    "PCC_final['Variable 2'] = var2\n",
    "PCC_final['PCC'] = PCC_score\n",
    "PCC_final = PCC_final.drop(PCC_final.index[(np.abs(PCC_final['PCC']) < 0.9) | (PCC_final['Variable 1'] == PCC_final['Variable 2'])].tolist(),\n",
    "                           axis=0)\n",
    "PCC_final.reset_index(drop=True, inplace=True)\n",
    "PCC_final.head(12)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains constructing DataFrame out of texts and cleaning them with regex, not shown here\n",
    "\n",
    "### Tf_df\n",
    "# Transform the text\n",
    "TfIDFTransformer = sktext.TfidfVectorizer(strip_accents='unicode', # Eliminate accents and special characters\n",
    "                      stop_words='english', # Eliminates stop words.\n",
    "                      min_df = 0.05, # Eliminate words that do not appear in more than 5% of texts\n",
    "                      max_df = 0.90, # Eliminate words that appear in more than 95% of texts\n",
    "                      sublinear_tf=True # Use sublinear weights (softplus), i.e., replace tf with 1 + log(tf)\n",
    "                      )\n",
    "TfIDF_IMDB = TfIDFTransformer.fit_transform(texts['texts']) # note that this is unsupervised. Also note this is sparse, call .toarray() method\n",
    "df1 = pd.DataFrame(TfIDF_IMDB.toarray(), columns=TfIDFTransformer.get_feature_names_out())\n",
    "df1.head()\n",
    "# Let's save the indexes for later.\n",
    "word_index = TfIDFTransformer.get_feature_names_out()\n",
    "print(word_index[30:40])\n",
    "#PCA_description.fit(csr_matrix(TfIDF_train).toarray()) for usage of csr_matrix() \n",
    "\n",
    "### pca example usage\n",
    "# Do normal PCA on the data set\n",
    "n = 2 \n",
    "nPCA = PCA(n_components=n, svd_solver='full')\n",
    "# Now we fit. We need to transform our matrix to dense format first.\n",
    "nPCA.fit(TfIDF_IMDB.toarray())\n",
    "# Let's calculate the variance of the two components. (lambda_i over sum of lambdas gives amount of explained variance)\n",
    "total_variance= np.sum(nPCA.explained_variance_)\n",
    "print('Total variance explained by the first %i components is %.3f.' % (nPCA.n_components_, total_variance))\n",
    "total_variance_ratio = np.sum(nPCA.explained_variance_ratio_)*100\n",
    "print('The first %i components explain %.3f%% of total variance.' % (nPCA.n_components_, total_variance_ratio))\n",
    "# Let's get the components and plot them, coloring by the class\n",
    "Z1 = nPCA.transform(TfIDF_IMDB.toarray())\n",
    "sns.scatterplot(x=Z1[:, 0], y=Z1[:, 1], hue=texts['class'])\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.title(\"PCA of IMDB texts\")\n",
    "plt.show()\n",
    "loadings = pd.DataFrame(nPCA.components_.T, columns=['PC1', 'PC2'], index=word_index) # nPCA.components_. is of shape comp x feat\n",
    "# Words that are positively correlated to the second component \n",
    "word_index[nPCA.components_[1,:] > 0]\n",
    "\n",
    "\n",
    "### nunpy linalg svd\n",
    "X = TfIDF_IMDB.toarray()\n",
    "X = X - X.mean(axis=0)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "u, s, vt = np.linalg.svd(X, full_matrices=False, compute_uv=True) # It's not necessary to compute the full matrix of U or V\n",
    "# flip eigenvectors' sign to enforce deterministic output\n",
    "def svd_flip(u, v):\n",
    "        max_abs_cols = np.argmax(np.abs(u), axis=0)\n",
    "        signs = np.sign(u[max_abs_cols, range(u.shape[1])])\n",
    "        u *= signs\n",
    "        v *= signs[:, np.newaxis]\n",
    "        return u, v\n",
    "\n",
    "u, vt = svd_flip(u, vt)\n",
    "\n",
    "\n",
    "### Sparse PCA\n",
    "# Now do sparse PCA enforcing sparseness on variables\n",
    "# that means only a few of the original variables can appear on each latent factor \n",
    "sPCA = SparsePCA(n_components=2, random_state=seed, alpha=0.1) # play with alpha to control sparsity. Higher values lead to sparser components. \n",
    "sPCA.fit(TfIDF_IMDB.toarray())\n",
    "# Get the results\n",
    "Z2 = sPCA.transform(TfIDF_IMDB.toarray()) # has shape feat/sample x comp\n",
    "# Create plot\n",
    "sns.scatterplot(x=Z2[:, 0], y=Z2[:, 1], hue=texts['class'])\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.title(\"Sparse PCA of IMDB texts\")\n",
    "plt.show()\n",
    "\n",
    "### via sklearn TruncatedSVD()\n",
    "svd = TruncatedSVD(n_components=100, # How many concepts to extract\n",
    "                   n_iter=10, # How many iterations to run\n",
    "                   random_state=seed # Random state. As SVD is rotation-invariant, we need to set this\n",
    "                  )\n",
    "svd.fit(TfIDF_IMDB)\n",
    "\n",
    "### TSNE\n",
    "tSNEmapper = TSNE(n_components=2,               # How many dimensions to use. Never more than 2 or 3\n",
    "                  init='random',                # First initialization. Sparse matrices need 'random'.  Otherwise use 'pca'\n",
    "                  perplexity=50.0,              # Read below\n",
    "                  early_exaggeration=12.0,      # Read below\n",
    "                  learning_rate='auto',         # Related to above. Leave to auto\n",
    "                  n_iter=5000,                  # Very important to let iterate enough\n",
    "                  n_iter_without_progress=300,  # Set early stopping\n",
    "                  metric='euclidean',           # Metric to use to calculate distances.\n",
    "                  min_grad_norm=1e-7,           # Minimum gradient to continue iterating\n",
    "                  verbose=0,                    # Verbosity\n",
    "                  random_state=seed,            # Random seed\n",
    "                  n_jobs=-1,                    # Parallel processes\n",
    "                 )\n",
    "TSNE_embedding = tSNEmapper.fit_transform(TfIDF_IMDB)\n",
    "sns.scatterplot(x=TSNE_embedding[:, 0], y=TSNE_embedding[:, 1], hue=texts['class'])\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.title(\"t-SNE projection of IMDB texts\")\n",
    "plt.show()\n",
    "\n",
    "### UMAP\n",
    "# Let's create the object\n",
    "reducer = umap.UMAP(n_neighbors=15,              # Number of neareast neighbours to use. \n",
    "                    n_components=2,              # Number of components. UMAP is robust to larger values\n",
    "                    metric='hellinger',          # Metric to use.\n",
    "                    n_epochs=None,               # Iterations. Set to convergence. None implies either 200 or 500.\n",
    "                    min_dist=0.1,                # Minimum distance embedded points. Smaller makes clumps, larger, sparseness.\n",
    "                    spread=1.0,                  # Scale to combine with min_dist\n",
    "                    low_memory=True,             # Run slower, but with less memory.\n",
    "                    n_jobs=-1,                   # Cores to use\n",
    "                    random_state=seed,           # Random seed\n",
    "                    verbose=False                # Verbosity\n",
    "                   )\n",
    "\n",
    "# Now we train and calculate the embedding!\n",
    "UMAP_embedding = reducer.fit_transform(TfIDF_IMDB) # don't forget to only transform the test set!\n",
    "# Create plot\n",
    "sns.scatterplot(x=UMAP_embedding[:, 0], y=UMAP_embedding[:, 1], hue=texts['class'])\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.title(\"UMAP of IMDB texts\")\n",
    "plt.show()\n",
    "# or\n",
    "umap.plot.points(reducer, labels=texts['class'])\n",
    "plt.show()\n",
    "\n",
    "### get most N=10 important features\n",
    "# Get 10 most important words for each component [ / 4 marks]\n",
    "words_per_row = TfIDFTransformer.get_feature_names_out()\n",
    "most_important = [np.argpartition(np.abs(pcamodel.components_[i]), -10)[-10:] for i in range(3)]\n",
    "\n",
    "### using itertools.product for various distance and neighbours in UMAP\n",
    "# Set parameters\n",
    "fig, axs = plt.subplots(5, 4, figsize=(30,30))\n",
    "n_neighbors=[2, 10, 25, 35, 45]\n",
    "min_dist=[0.1, 0.25, 0.5, 1]\n",
    "fig.tight_layout()\n",
    "sns.set_style('white')\n",
    "# Create UMAP and plots [ / 8 marks]\n",
    "for (i,j), (nei, dist) in zip(product([0,1,2,3,4],[0,1,2,3]), product(n_neighbors, min_dist)):\n",
    "    UMAP_description = umap.UMAP(n_components = 2,\n",
    "                             metric='cosine',\n",
    "                             n_epochs=1000,\n",
    "                             low_memory=False,\n",
    "                             n_neighbors=nei,\n",
    "                             spread=1,\n",
    "                             min_dist=dist,\n",
    "                             init='random'\n",
    "                            )\n",
    "\n",
    "    x_train_umap = UMAP_description.fit_transform(TfIDF_train)\n",
    "\n",
    "    # Create plot\n",
    "    sns.scatterplot(x=x_train_umap[:, 0], y=x_train_umap[:, 1], hue=y_train, ax=axs[i,j])\n",
    "    axs[i,j].title.set_text(f\"n_neighbors={nei}, min_dist={dist}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### xgb.cv() for best iteration (don't forget to convert data to Dmatrices first!)\n",
    "# Define XGBoost parameters\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"max_depth\": 3,\n",
    "}\n",
    "\n",
    "# Define cross-validation object\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "# Perform cross-validation with XGBoost [ \\3 marks]\n",
    "cv_results = xgb.cv(\n",
    "    params=params,\n",
    "    dtrain=xgb.DMatrix(x_train_pca, label=y_train),\n",
    "    num_boost_round=10,\n",
    "    folds=cv,\n",
    "    metrics=[\"auc\"],\n",
    "    early_stopping_rounds=2,\n",
    "    seed=42\n",
    ")\n",
    "# Get best iteration based on cross-validation results [\\ 1 mark]\n",
    "best_iteration = cv_results[\"test-auc-mean\"].argmax()\n",
    "# Train final model on full dataset with best number of iterations [\\ 2 mark]\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=xgb.DMatrix(x_train_pca, label=y_train),\n",
    "    num_boost_round=best_iteration+1\n",
    ")\n",
    "# Compute predicted probabilities on the test set [\\ 1 mark]\n",
    "dtest = xgb.DMatrix(x_test_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KMeans()\n",
    "# Initializing the cluster algorithm\n",
    "KClusterer = KMeans(n_clusters=3) # Name of operator and cluster number\n",
    "# Fit the cluster and predict the label in one step. Calls any preprocessing step plus model\n",
    "df['cluster_label'] = KClusterer.fit_predict(X)\n",
    "\n",
    "### Aglomerative()\n",
    "# Now a pipeline with the agglomerative cluster.\n",
    "AgglomerativeIris = AgglomerativeClustering(n_clusters=4,         # Number of clusters\n",
    "                                            affinity='euclidean', # Type of distance. Depends on your data and you can create your own!\n",
    "                                            linkage=\"single\"      # Type of linkage.  \n",
    "                                            )\n",
    "\n",
    "df['cluster_label'] = AgglomerativeIris.fit_predict(X)\n",
    "\n",
    "# Crosstab with the averages per cluster\n",
    "df.groupby(['KMeans_Clusters'])['Age','Work_Experience','Family_Size'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable importance by mean-shap values\n",
    "explainer = shap.Explainer(GridXGB.best_estimator_)\n",
    "shap_values = explainer(x_test)\n",
    "\n",
    "# Report the average importances.\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "#per case\n",
    "shap.plots.waterfall(shap_values[0])\n",
    "shap.plots.force(shap_values[0], matplotlib=True) # same line\n",
    "\n",
    "# Scatter, shap values for var vs. values of var, color is most correlated variable\n",
    "shap.plots.scatter(shap_values[:,\"Term\"], color=shap_values)\n",
    "\n",
    "# all in one\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3018ec948c2f097dac52d5fcdb1a30ae92fa49919476c7ddb718ffa59f845b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
